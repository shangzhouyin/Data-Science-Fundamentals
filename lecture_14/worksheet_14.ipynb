{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 14\n",
    "\n",
    "Name:  \n",
    "UID: \n",
    "\n",
    "### Topics\n",
    "\n",
    "- Naive Bayes\n",
    "- Model Evaluation\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "| Attribute A | Attribute B | Attribute C | Class |\n",
    "|-------------|-------------|-------------|-------|\n",
    "| Yes         | Single      | High        | No    |\n",
    "| No          | Married     | Mid         | No    |\n",
    "| No          | Single      | Low         | No    |\n",
    "| Yes         | Married     | High        | No    |\n",
    "| No          | Divorced    | Mid         | Yes   |\n",
    "| No          | Married     | Low         | No    |\n",
    "| Yes         | Divorced    | High        | No    |\n",
    "| No          | Single      | Mid         | Yes   |\n",
    "| No          | Married     | Low         | No    |\n",
    "| No          | Single      | Mid         | Yes   |\n",
    "\n",
    "a) Compute the following probabilities:\n",
    "\n",
    "- P(Attribute A = Yes | Class = No)\n",
    "- P(Attribute B = Divorced | Class = Yes)\n",
    "- P(Attribute C = High | Class = No)\n",
    "- P(Attribute C = Mid | Class = Yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3/7\n",
    "2.1\n",
    "3.0\n",
    "4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Classify the following unseen records:\n",
    "\n",
    "- (Yes, Married, Mid)\n",
    "- (No, Divorced, High)\n",
    "- (No, Single, High)\n",
    "- (No, Divorced, Low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p(yes | c= yes)P(married | c = yes)p(mid|c = yes)p(yes)\n",
    "vs\n",
    "p(yes | c= no)P(married | c = no)p(mid|c = no)p(no)\n",
    "\n",
    "p(No | c= yes)P(Divorced| c = yes)p(High|c = yes)p(yes)\n",
    "vs\n",
    "p(No | c= no)P(Divorced| c = no)p(High|c = no)p(no)\n",
    "\n",
    "p(No | c= yes)P(Single| c = yes)p(Low|c = yes)p(yes)\n",
    "vs\n",
    "p(No | c= no)P(Single| c = no)p(Low|c = no)p(no)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "a) Write a function to generate the confusion matrix for a list of actual classes and a list of predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "actual_class = [\"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No\"]\n",
    "predicted_class = [\"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\"]\n",
    "\n",
    "def confusion_matrix(actual, predicted):\n",
    "    true_positive = true_negative = false_positive = false_negative = 0\n",
    "\n",
    "    for a, p in zip(actual, predicted):\n",
    "        if a == \"Yes\" and p == \"Yes\":\n",
    "            true_positive += 1\n",
    "        elif a == \"No\" and p == \"No\":\n",
    "            true_negative += 1\n",
    "        elif a == \"No\" and p == \"Yes\":\n",
    "            false_positive += 1\n",
    "        elif a == \"Yes\" and p == \"No\":\n",
    "            false_negative += 1\n",
    "\n",
    "    return {\n",
    "        \"True Positive\": true_positive,\n",
    "        \"False Negative\": false_negative,\n",
    "        \"False Positive\": false_positive,\n",
    "        \"True Negative\": true_negative\n",
    "    }\n",
    "\n",
    "\n",
    "print(confusion_matrix(actual_class, predicted_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Assume you have the following Cost Matrix:\n",
    "\n",
    "|            | predicted = Y | predicted = N |\n",
    "|------------|---------------|---------------|\n",
    "| actual = Y |       -1      |       5       |\n",
    "| actual = N |        10     |       0       |\n",
    "\n",
    "What is the cost of the above classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Write a function that takes in the actual values, the predictions, and a cost matrix and outputs a cost. Test it on the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(actual, predicted, cost_matrix):\n",
    "    confusion_values = calc_confusion_matrix(actual, predicted)\n",
    "    cost = cost_matrix[\"Yes\"][\"Yes\"] * confusion_values['True Positive'] + cost_matrix[\"Yes\"][\"No\"] * confusion_values['False Negative'] + cost_matrix[\"No\"][\"Yes\"] * confusion_values['False Positive'] + cost_matrix[\"No\"][\"No\"] * confusion_values['True Negative']\n",
    "    return cost\n",
    "\n",
    "\n",
    "cost_matrix = {\n",
    "    \"Yes\": {\"Yes\": -1, \"No\": 5},\n",
    "    \"No\": {\"Yes\": 10, \"No\": 0}\n",
    "}\n",
    "\n",
    "cost=calculate_cost(actual_class, predicted_class,cost_matrix)\n",
    "\n",
    "print(\"Cost of classification : \", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Implement functions for the following:\n",
    "\n",
    "- accuracy\n",
    "- precision\n",
    "- recall\n",
    "- f-measure\n",
    "\n",
    "and apply them to the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(confusion_matrix_v):\n",
    "    tp= confusion_matrix_v['True Positive']\n",
    "    fp= confusion_matrix_v['False Positive']\n",
    "    tn= confusion_matrix_v['True Negative']\n",
    "    fn= confusion_matrix_v['False Negative']\n",
    "\n",
    "\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "    p = tp/(tp+fp)\n",
    "    r=tp/(tp+fn)\n",
    "    fmeasure=(2*p*r)/(p+r)\n",
    "\n",
    "    return accuracy, p,r, fmeasure\n",
    "\n",
    "\n",
    "acc,precision,recall,f1score=metrics(confusion)\n",
    "print('Accuracy = ',acc*100,\"%\")\n",
    "print('Precision = ',precision)\n",
    "print('Recall = ',recall)\n",
    "print('F-measure = ',f1score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
